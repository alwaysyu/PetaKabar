{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library-library\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "# Data Preparation and Preprocessing\n",
    "import pandas as pd\n",
    "import re\n",
    "from string import digits\n",
    "\n",
    "# Word Embedding\n",
    "import joblib\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('xlm-r-distilroberta-base-paraphrase-v1')\n",
    "from keybert import KeyBERT\n",
    "kw_extractor = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input and Expansion Query\n",
    "import nltk\n",
    "import math\n",
    "from textblob import TextBlob\n",
    "from yake import KeywordExtractor\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize\n",
    "from operator import itemgetter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_words = [\"tempat\", \"waktu\"]\n",
    "\n",
    "NLTK_StopWords = stopwords.words('indonesian')\n",
    "NLTK_StopWords.extend([\"detik\", \"detikjatim\", \"detikjateng\", \"detikjabar\", \"detiksulsel\", \"detiksumbar\", \"detikbali\", \"detikpapua\", \"detiksulteng\", \"detikmaluku\", \"detjatim\", \"detikcom\", \"allahumma\", \"aamiin\", \"allah\", \"bismillah\"])\n",
    "NLTK_StopWords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', \n",
    "                       'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih', \n",
    "                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "                       'jd', 'jgn', 'sdh', 'aja', 'n', 't', \n",
    "                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                       '&amp', 'yah'])\n",
    "txt_stopword = pd.read_csv(\"stopwords.txt\", names= [\"stopwords\"], header = None)\n",
    "\n",
    "# convert stopword string to list & append additional stopword\n",
    "NLTK_StopWords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
    "NLTK_StopWords = set(NLTK_StopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocessing(berita):\n",
    "#     # Preprocessing\n",
    "#     s = berita.lower()\n",
    "#     s = s.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    "#     s = s.encode('ascii', 'replace').decode('ascii')\n",
    "#     ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", s).split())\n",
    "#     s.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "#     s = re.sub('\\s+', ' ', s)\n",
    "#     s = s.strip()\n",
    "#     s = s.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "#     s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "#     tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "\n",
    "#     # Stopwords checking\n",
    "#     T = [t for t in tokens if ((t in excluded_words) or (t not in NLTK_StopWords))]\n",
    "#     return T\n",
    "def preprocessing(berita):\n",
    "    s = str(berita)\n",
    "    s = s.lower()\n",
    "    s = s.replace('\\n', ' ')\n",
    "    s = s.replace('\\r', ' ')\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    T = [t for t in tokens if ((t in excluded_words) or (t not in NLTK_StopWords))]\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1612 entries, 0 to 1611\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   title        1612 non-null   object\n",
      " 1   date         1612 non-null   object\n",
      " 2   description  1612 non-null   object\n",
      " 3   source       1612 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 63.0+ KB\n",
      "None\n",
      "------------------------------------------------------------------------------------------\n",
      "1450\n"
     ]
    }
   ],
   "source": [
    "df_total = pd.read_csv('corpus/dataset/df_total.csv')\n",
    "df_total = df_total[pd.notnull(df_total['description'])]\n",
    "print(df_total.info())\n",
    "print ('-'*90)\n",
    "document_text= joblib.load('corpus/model/desc_text_train.pkl')\n",
    "print(len(document_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 162 entries, 0 to 161\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   title        162 non-null    object\n",
      " 1   date         162 non-null    object\n",
      " 2   description  162 non-null    object\n",
      " 3   source       162 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 6.3+ KB\n",
      "None\n",
      "------------------------------------------------------------------------------------------\n",
      "162\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('corpus/dataset/df_test.csv')\n",
    "df_test = df_test[pd.notnull(df_test['description'])]\n",
    "print(df_test.info())\n",
    "print ('-'*90)\n",
    "document_text_test= joblib.load('corpus/model/desc_text_test.pkl')\n",
    "print(len(document_text_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1450 entries, 0 to 1449\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   title        1450 non-null   object\n",
      " 1   date         1450 non-null   object\n",
      " 2   description  1450 non-null   object\n",
      " 3   source       1450 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 56.6+ KB\n",
      "None\n",
      "------------------------------------------------------------------------------------------\n",
      "1450\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('corpus/dataset/df_train.csv')\n",
    "df_train = df_train[pd.notnull(df_train['description'])]\n",
    "print(df_train.info())\n",
    "print ('-'*90)\n",
    "document_text_train= joblib.load('corpus/model/desc_text_train.pkl')\n",
    "print(len(document_text_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tingkat setelah parent</th>\n",
       "      <th>parent</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>bencana</td>\n",
       "      <td>[('musibah', 0.6435083150863647), ('banjir', 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>musibah</td>\n",
       "      <td>[('kecelakaan', 0.7218754887580872), ('insiden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>banjir</td>\n",
       "      <td>[('longsor', 0.7020419836044312), ('kekeringan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>kelaparan</td>\n",
       "      <td>[('wabah', 0.7254995703697205), ('epidemi', 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>gempa</td>\n",
       "      <td>[('tsunami', 0.6986302137374878), ('letusan', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tingkat setelah parent     parent  \\\n",
       "0                       0    bencana   \n",
       "1                       1    musibah   \n",
       "2                       1     banjir   \n",
       "3                       1  kelaparan   \n",
       "4                       1      gempa   \n",
       "\n",
       "                                          similarity  \n",
       "0  [('musibah', 0.6435083150863647), ('banjir', 0...  \n",
       "1  [('kecelakaan', 0.7218754887580872), ('insiden...  \n",
       "2  [('longsor', 0.7020419836044312), ('kekeringan...  \n",
       "3  [('wabah', 0.7254995703697205), ('epidemi', 0....  \n",
       "4  [('tsunami', 0.6986302137374878), ('letusan', ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load bow dataset\n",
    "df_bow_what = pd.read_csv(\"bow/bow_what.csv\")\n",
    "df_bow_what.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What  bencana\n",
      "{'What': 321}\n"
     ]
    }
   ],
   "source": [
    "# Ambil parent dari bow\n",
    "bow_list_what = []\n",
    "\n",
    "for i in range(0, df_bow_what.shape[0]):\n",
    "  bow_list_what.append(df_bow_what.iloc[i, 1])\n",
    "\n",
    "print(\"What \", bow_list_what[0])\n",
    "\n",
    "print({\n",
    "  'What': len(bow_list_what),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cari dok pertama Use data train\n",
    "def cari_dokpertama(kueriAsli: str):\n",
    "    kueriPre = preprocessing(kueriAsli)\n",
    "    kueriPre = \" \".join(kueriPre)\n",
    "    hasilSearch = []\n",
    "    tfidf_matrix = joblib.load('corpus/matrix/tfidf_train.pkl')\n",
    "    tfidf_vectorizer = joblib.load('corpus/vectorizer/vectorizer.pkl')\n",
    "    query_vec = tfidf_vectorizer.transform([kueriPre])\n",
    "    results = cosine_similarity(tfidf_matrix, query_vec).reshape((-1))\n",
    "    for i in results.argsort()[-5:][::-1]:\n",
    "        hasilSearch.append(df_total.iloc[i,-2])\n",
    "    hasilSearch=\". \".join(hasilSearch)\n",
    "    \n",
    "    return hasilSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Keywords Extraction with YAKE\n",
    "def keyword_yake(hasilSearch: str):\n",
    "    keywordYake=[]\n",
    "\n",
    "    k_extractor = KeywordExtractor(lan=\"id\", n=1, top=50)\n",
    "    k_extractor2 = KeywordExtractor(lan=\"id\", n=2, top=50)\n",
    "    keywords = k_extractor.extract_keywords(text=hasilSearch)\n",
    "    # keywords = k_extractor2.extract_keywords(text=hasilSearch)\n",
    "    keywords.extend(k_extractor2.extract_keywords(text=hasilSearch))\n",
    "    keywordYake = [x for x, y in keywords]\n",
    "    # keywordYake.append(keywords)\n",
    "    print('*'*120)\n",
    "    print('*'*120)\n",
    "    print(\"Keyword yake\")\n",
    "    print(keywordYake)\n",
    "    print('*'*120)\n",
    "    print('*'*120)\n",
    "    \n",
    "    return keywordYake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keywords Extraction with TFIDF\n",
    "def keyword_tfidf(hasilSearch: str):\n",
    "\n",
    "    keywordtfidf=[]\n",
    "    keywordtfidf2=[]\n",
    "\n",
    "    total_words = re.sub(r'[^\\w]', ' ', hasilSearch)\n",
    "    total_words = total_words.lower().split()\n",
    "    #print (total_words)\n",
    "    total_word_length = len(total_words)\n",
    "    total_sentences = tokenize.sent_tokenize(hasilSearch)\n",
    "    total_sent_len = len(total_sentences)\n",
    "\n",
    "    tf_score = {}\n",
    "    for each_word in total_words:\n",
    "        #print (each_word)\n",
    "        each_word = each_word.replace('.','')\n",
    "        if (each_word in excluded_words) or (each_word not in NLTK_StopWords):\n",
    "            if each_word in tf_score:\n",
    "                tf_score[each_word] += 1\n",
    "            else:\n",
    "                tf_score[each_word] = 1\n",
    "\n",
    "    # Dividing by total_word_length for each dictionary element\n",
    "    tf_score.update((x, y/int(total_word_length)) for x, y in tf_score.items())\n",
    "    #print(tf_score)\n",
    "    def check_sent(word, sentences): \n",
    "        final = [all([w in x for w in word]) for x in sentences] \n",
    "        sent_len = [sentences[i] for i in range(0, len(final)) if final[i]]\n",
    "        return int(len(sent_len))\n",
    "\n",
    "    idf_score = {}\n",
    "    for each_word in total_words:\n",
    "        #print (each_word)\n",
    "        each_word = each_word.replace('.','')\n",
    "        if (each_word in excluded_words) or (each_word not in NLTK_StopWords):\n",
    "            if each_word in idf_score:\n",
    "                idf_score[each_word] = check_sent(each_word, total_sentences)\n",
    "            else:\n",
    "                idf_score[each_word] = 1\n",
    "\n",
    "    # Performing a log and divide\n",
    "    idf_score.update((x, math.log(int(total_sent_len)/y)) for x, y in idf_score.items())\n",
    "\n",
    "    #print(idf_score)\n",
    "    tf_idf_score = {key: tf_score[key] * idf_score.get(key, 0) for key in tf_score.keys()}\n",
    "    #print(tf_idf_score)\n",
    "    def get_top_n(dict_elem, n):\n",
    "        result = dict(sorted(dict_elem.items(), key = itemgetter(1), reverse = True)[:n]) \n",
    "        hasil =list(result.keys())\n",
    "        #print(list(result.keys()))        \n",
    "        return hasil\n",
    "    #print(get_top_n(tf_idf_score, 25))\n",
    "    #print(len(get_top_n(tf_idf_score, 1)))\n",
    "    keywordtfidf.append(get_top_n(tf_idf_score, 25))\n",
    "    for i in range(len(keywordtfidf)):\n",
    "        #print (i)\n",
    "        totalKw=0\n",
    "        totalKw=len(keywordtfidf[i])\n",
    "        for j in range(totalKw):\n",
    "            #print (j)\n",
    "            keywordtfidf2.append(keywordtfidf[i][j])\n",
    "    \n",
    "    print('*'*120)\n",
    "    print('*'*120)\n",
    "    print(\"Keyword TFIDF\")\n",
    "    print (keywordtfidf2)\n",
    "    print('*'*120)\n",
    "    print('*'*120)\n",
    "\n",
    "    return keywordtfidf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keywords Extraction with BERT\n",
    "def keyword_bert(hasilSearch: str):\n",
    "\n",
    "    keywordbert=[]\n",
    "\n",
    "    #for j in range(len(array_text)):\n",
    "    keyword1 = kw_extractor.extract_keywords(hasilSearch, top_n=50, keyphrase_ngram_range=(1, 1))\n",
    "    keyword2 = kw_extractor.extract_keywords(hasilSearch, top_n=50, keyphrase_ngram_range=(1, 2))\n",
    "\n",
    "    #print(\"Keywords of article\\n\", keywords)\n",
    "    for i in range (0,len (keyword1)):\n",
    "        keywordbert.append(keyword1[i][0])\n",
    "        keywordbert.append(keyword2[i][0])\n",
    "    \n",
    "    print('*'*120)\n",
    "    print('*'*120)\n",
    "    print(\"Keyword Bert\")\n",
    "    print (keywordbert)\n",
    "    print('*'*120)\n",
    "    print('*'*120)\n",
    "    \n",
    "    return keywordbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Borda rangking\n",
    "def rangking (keywordGabung, kueriAsli: str):\n",
    "    kandidatFinalCek=[]\n",
    "    kandidatFinalFix=[]\n",
    "    \n",
    "    for i in keywordGabung:\n",
    "        if (i not in kandidatFinalCek and i!=0):\n",
    "            kandidatFinalCek.append(i)\n",
    "    queries=[kueriAsli]\n",
    "    query_embeddings = embedder.encode(queries)\n",
    "    corpus_embeddings4 = embedder.encode(kandidatFinalCek)\n",
    "    # Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "    closest_n = 80\n",
    "    for query, query_embedding in zip(queries, query_embeddings):\n",
    "        distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings4, 'cosine')[0]\n",
    "        results = zip(range(len(distances)), distances)\n",
    "        results = sorted(results, key=lambda x: x[1])\n",
    "        for idx, distance in results[0:closest_n]:\n",
    "            kandidatFinalFix.append(kandidatFinalCek[idx])\n",
    "\n",
    "    print('*'*120)\n",
    "    print('*'*120)\n",
    "    print ('Kandidat Final Fix Rank: ', kandidatFinalFix)\n",
    "    print('*'*120)\n",
    "    print('*'*120)\n",
    "\n",
    "    return kandidatFinalFix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword bow\n",
    "def keywordCustomBow(bowList, initialQuery: str):\n",
    "    cekDuplicate = []\n",
    "    kandidatFix = []\n",
    "\n",
    "    for i in bowList:\n",
    "        if(i not in cekDuplicate and i!=0):\n",
    "            cekDuplicate.append(i)\n",
    "\n",
    "    queries=[initialQuery]\n",
    "    query_embeddings = embedder.encode(queries)\n",
    "    corpus_embeddings4 = embedder.encode(cekDuplicate)\n",
    "    \n",
    "    # Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "    closest_n = 500\n",
    "    for query, query_embedding in zip(queries, query_embeddings):\n",
    "        distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings4, 'cosine')[0]\n",
    "        results = zip(range(len(distances)), distances)\n",
    "        results = sorted(results, key=lambda x: x[1])\n",
    "        for idx, distance in results[0:closest_n]:\n",
    "            kandidatFix.append(cekDuplicate[idx])\n",
    "    \n",
    "    print('*'*120)\n",
    "    print('*'*120)\n",
    "    print(\"Keyword BoW\")\n",
    "    print ('Kandidat BoW: ', kandidatFix)\n",
    "    print('*'*120)\n",
    "    print('*'*120)\n",
    "\n",
    "    return kandidatFix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bencana berita\n"
     ]
    }
   ],
   "source": [
    "# Creating query for what, when, where, who\n",
    "what_initial_query = \"bencana apa yang terjadi dalam berita\"\n",
    "\n",
    "what_query = preprocessing(what_initial_query)\n",
    "what_query = \" \".join(what_query)\n",
    "print (what_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare w data\n",
    "def prepareWData(initial_query: str, bow_list):\n",
    "    hasilkandidat = []\n",
    "    keywordGabung = []\n",
    "    qeGabungan = []\n",
    "    kueriFix = []\n",
    "\n",
    "    hasilSearch     = cari_dokpertama(initial_query)\n",
    "    # (ini yake + tfidf + bert) = qe statistik\n",
    "    keywordYake     = keyword_yake(hasilSearch) # 20\n",
    "    keywordtfidf2   = keyword_tfidf(hasilSearch) # 20\n",
    "    keywordbert     = keyword_bert(hasilSearch) # 20\n",
    "    # ini qe bow\n",
    "    keywordBoW      = keywordCustomBow(bow_list, initial_query)\n",
    "\n",
    "    for keyword1 in keywordYake:\n",
    "        keywordGabung.append(keyword1)\n",
    "    for keyword2 in keywordtfidf2:\n",
    "        keywordGabung.append(keyword2)\n",
    "    for keyword3 in keywordbert:\n",
    "        keywordGabung.append(keyword3)  \n",
    "\n",
    "    # hasilrank = qe statistik\n",
    "    hasilrank = rangking(keywordGabung, initial_query)\n",
    "    \n",
    "    for word1 in hasilrank:\n",
    "        kueriFix.append(word1)\n",
    "\n",
    "    for word2 in keywordBoW:\n",
    "        kueriFix.append(word2)\n",
    "\n",
    "    for word3 in kueriFix:\n",
    "        hasilkandidat.append(word3)\n",
    "\n",
    "    kueriFix = [preprocessing(i) for i in kueriFix]\n",
    "    \n",
    "    qeGabunganDelimiter = []\n",
    "\n",
    "    for word4 in kueriFix:\n",
    "        for subWord in word4:\n",
    "            qeGabungan.append(subWord)\n",
    "            qeGabunganDelimiter.append(subWord)\n",
    "\n",
    "    qeGabunganDelimiter = list(dict.fromkeys(qeGabunganDelimiter))\n",
    "    qeGabungan = list(dict.fromkeys(qeGabungan))\n",
    "\n",
    "    # (hasil ranking + bow) = kandidat final\n",
    "    qeGabungan = [\" \".join(qeGabungan)]\n",
    "\n",
    "    print('*'*120)\n",
    "    qeStatistik = hasilrank\n",
    "    qeBoW = keywordBoW\n",
    "\n",
    "    return [qeGabungan, qeStatistik, qeBoW, qeGabunganDelimiter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************************************************************\n",
      "************************************************************************************************************************\n",
      "Keyword yake\n",
      "['banjir', 'hujan', 'air', 'kawasan', 'warga', 'jakarta', 'rumah', 'sungai', 'tanggul', 'gempa', 'wib', 'timur', 'kramat', 'jati', 'ciliwung', 'kondisi', 'hek', 'pertigaan', 'juni', 'annisa', 'bogor', 'laut', 'dirasakan', 'getaran', 'rizky', 'barat', 'andi', 'luapan', 'salahuddin', 'puncak', 'video', 'desa', 'jembatan', 'pusat', 'gambas', 'hanyut', 'waspada', 'debit', 'deras', 'katulampa', 'lokasi', 'mengguyur', 'manokwari', 'skala', 'iii', 'mmi', 'benda', 'intensitas', 'pantauan', 'fadhila', 'kramat jati', 'jati jakarta', 'sungai ciliwung', 'hek kramat', 'jakarta timur', 'getaran dirasakan', 'pertigaan hek', 'kawasan pertigaan', 'tanggul kawasan', 'banjir', 'annisa rizky', 'hujan', 'air', 'kawasan', 'warga', 'kondisi tanggul', 'gambas video', 'jakarta', 'rumah', 'sungai', 'air sungai', 'gempa laut', 'pusat gempa', 'kawasan puncak', 'tanggul', 'gempa', 'laut manokwari', 'dirasakan skala', 'mmi getaran', 'dirasakan benda', 'iii getaran', 'hujan deras', 'wib', 'debit air', 'timur', 'kramat', 'jati', 'skala iii', 'iii mmi', 'benda benda', 'skala mmi', 'mmi iii', 'getaran seakan', 'ciliwung', 'kondisi', 'rizky fadhila', 'luapan sungai', 'hek', 'pertigaan', 'barat laut']\n",
      "************************************************************************************************************************\n",
      "************************************************************************************************************************\n",
      "************************************************************************************************************************\n",
      "************************************************************************************************************************\n",
      "Keyword TFIDF\n",
      "['rizky', 'hujan', '9', 'fadhila', 'fadhiladetikcom', 'rumah', '8', '17', 'bermagnitudo', 'papua', '53', 'tulis', 'akun', 'twitternya', 'infobmkg', '23', '54', '0', '73', '133', '60', 'ringan', 'digantung', 'bergoyang', 'seakan']\n",
      "************************************************************************************************************************\n",
      "************************************************************************************************************************\n",
      "************************************************************************************************************************\n",
      "************************************************************************************************************************\n",
      "Keyword Bert\n",
      "['penghubung', 'fadhiladetikcom jpeg', 'jpeg', 'jpeg 4032', '109', 'selatan 133', 'keluarga', 'meningkat 16', 'kabupaten', '722 rumah', 'kecamatan', '2022 pusat', 'pengerjaannya', 'mengguyur rahayu', 'tenggara', 'hanyut 722', 'tutupnya', 'warga kabupaten', 'pembangunan', 'penghubung antardusun', 'kelurahan', 'penghubung antardesa', 'mengguyur', 'warga pembangunan', 'salahuddin', 'mencapai 100', 'fadhiladetikcom', 'penghubung desa', 'mengimbau', 'ls 109', 'lintang', 'bambangan kecamatan', 'berharapnya', 'tenggara cilacap', 'banjirnya', 'pemukiman warga', 'mencapai', 'rabu 2022', 'menyambung', 'kabupaten cilacap', 'rahayu', 'gempa 06', 'wartawan', 'keluarga pengungsian', '2022', 'ciliwung meningkat', 'akibatnya', 'menembus tanggul', 'hulu', 'beton penghubung', 'pusat', 'mengguyur kabupaten', 'luapan', 'telepon jumat', 'dipicu', 'pungkasnya 772', 'bermagnitudo', '2022 berharap', 'tumpah', 'antardesa kabupaten', '4032', 'rumah keluarga', 'kemarin', 'bertambah 100', 'diperparah', 'pembangunan tanggul', '133', 'luapan terang', 'keruh', 'berjarak 63', '53', 'mengaku menerima', 'mengaku', 'mengimbau warga', 'menerima', '4032 kondisi', 'bergoyang', '2022 gempa', '20detik', 'pemukiman banjir', 'bunyi', 'digantung bergoyang', 'kerusakan', '2022 kondisi', 'pungkasnya', 'warga pemerintah', 'meningkat', 'angin kelurahan', '722', 'sambungan telepon', 'pemukiman', 'petugas jaga', 'langganan', 'banjirnya ngguyur', '54', 'kecamatan malunda', 'jaga', 'menyebut warga', 'telepon', 'bt berjarak']\n",
      "************************************************************************************************************************\n",
      "************************************************************************************************************************\n",
      "************************************************************************************************************************\n",
      "************************************************************************************************************************\n",
      "Keyword BoW\n",
      "Kandidat BoW:  ['bencana', 'malapetaka', 'musibah', 'kebinasaan', 'kelangkaan', 'kecemasan', 'kesukaran', 'letusan', 'benturan', 'kemusnahan', 'fitnah', 'kemalangan', 'kekacauan', 'sengsara', 'penyalaan', 'keruntuhan', 'kegelisahan', 'peristiwa', 'kericuhan', 'badai', 'kejadian', 'pergolakan', 'keributan', 'gempabumi', 'kelaparan', 'kerugian', 'kemunduran', 'rawan', 'kehancuran', 'banjir', 'tabrakan', 'letusannya', 'peledakan', 'pencairan', 'erupsi', 'kesesakan', 'kecelakaan', 'kesengsaraan', 'kepanikan', 'penyanderaan', 'kegemparan', 'penggenangan', 'keanehan', 'tertabrak', 'penampakan', 'penguapan', 'petaka', 'gempa', 'kesusahan', 'pengikisan', 'rusak', 'kemurkaan', 'kejadiannya', 'kesialan', 'persitiwa', 'kejatuhan', 'tubrukan', 'tertimpa', 'diselimuti', 'menghantam', 'pengendapan', 'penghancuran', 'kebutaan', 'durhaka', 'kekeringan', 'ledakan', 'panik', 'semburan', 'hurikan', 'eskalasi', 'kerusuhan', 'pemadaman', 'luapan', 'pemogokan', 'meledak', 'tertimbun', 'penumpukan', 'kerentanan', 'kekurangan', 'kerawanan', 'meluap', 'penyakit', 'berserakan', 'kesedihan', 'dihantam', 'kecelakan', 'kepekaan', 'imbasnya', 'pelebaran', 'wabah', 'insiden', 'keresahan', 'tersapu', 'malafungsi', 'penghinaan', 'penderitaan', 'hambatan', 'seismik', 'ditimpa', 'genangan', 'retakan', 'hancur', 'goncangan', 'malfungsi', 'kekejian', 'fenomena', 'korsleting', 'diratakan', 'menerjang', 'getaran', 'guncangan', 'ditimbun', 'provokasi', 'erosi', 'pemuaian', 'menerpa', 'godaan', 'penyusutan', 'murka', 'porak', 'tragedi', 'abrasi', 'rusaknya', 'bendungan', 'kutukan', 'selokan', 'epidemik', 'bencah', 'terendam', 'risiko', 'jebolnya', 'ketidak', 'topan', 'pedih', 'azab', 'badatu', 'susulan', 'kelud', 'mereda', 'kelowongan', 'bendung', 'diterjang', 'kemacetan', 'penyesalan', 'pendangkalan', 'sambaran', 'terpaan', 'berapi', 'bahaya', 'terjangan', 'menghanguskan', 'proteksi', 'lonjakan', 'penundaan', 'lantak', 'timbulnya', 'berlumpur', 'waduk', 'kekecewaan', 'glasier', 'skandal', 'tornado', 'ambruk', 'liatnya', 'disentri', 'kebodohan', 'surplus', 'dibakar', 'pelapukan', 'kutuk', 'roboh', 'diguncang', 'iritasi', 'jebol', 'terhempas', 'terparah', 'cobaan', 'kehebohan', 'kekhawatiran', 'pandemik', 'epidemi', 'bentrokan', 'menggenangi', 'hancurnya', 'tewas', 'pembantaian', 'tererosi', 'korosi', 'macet', 'penolakan', 'freatik', 'kebingungan', 'meledaknya', 'tipus', 'ketahanannya', 'kebakaran', 'kejutan', 'drainase', 'ketegangan', 'hantaman', 'cemas', 'kebencian', 'ketakutan', 'ketidaknyamanan', 'melanda', 'konsekuensi', 'dekompresi', 'sedimen', 'jatuhnya', 'ketidakstabilan', 'stress', 'intrusi', 'kecurigaan', 'toksisitas', 'keterlambatan', 'ketidakpuasan', 'ketidakpastian', 'taifun', 'campak', 'tembin', 'kebocoran', 'pandemi', 'cidera', 'terjangkit', 'deformasi', 'bom', 'kontaminasi', 'kesakitan', 'berkepanjangan', 'siksaan', 'ancaman', 'keberuntungan', 'variola', 'ketergantungan', 'dampak', 'longsor', 'jatuhan', 'terbakar', 'sedimentasi', 'meletus', 'terluka', 'tsunami', 'ketidakpedulian', 'kemarahan', 'meluapnya', 'amblas', 'takut', 'berisiko', 'salinisasi', 'mengguncang', 'lantakkan', 'tifus', 'parit', 'kompleksitas', 'kegemukan', 'dampaknya', 'longsoran', 'sensitivitas', 'demoralisasi', 'dilanda', 'celaka', 'penembakan', 'haiyan', 'subsiden', 'berkurangnya', 'kengerian', 'piroklastik', 'resistansi', 'tanggul', 'luap', 'siklon', 'risikonya', 'vulkanis', 'sedalam', 'resistensi', 'vulkanik', 'membakar', 'keruh', 'eutrofikasi', 'manupeu', 'lamping', 'dialaminya', 'marah', 'terkubur', 'vulkanisme', 'bandang', 'imbas', 'pencobaan', 'sareal', 'memburuknya', 'musuh', 'deforestasi', 'monsun', 'terbakarnya', 'menyala', 'lava', 'geram', 'pneumonia', 'patahan', 'organosol', 'sesar', 'lahar', 'berpotensi', 'amarah', 'kolera', 'malaria', 'pascagempa', 'tewasnya', 'terganggunya', 'ulah', 'terowongan', 'tuberkulosis', 'tektonik', 'ibuku', 'membakarnya', 'kecemburuan', 'anakku', 'cemburu', 'kemarahannya', 'kaldera', 'ayahmu']\n",
      "************************************************************************************************************************\n",
      "************************************************************************************************************************\n",
      "************************************************************************************************************************\n",
      "************************************************************************************************************************\n",
      "Kandidat Final Fix Rank:  ['banjir', 'kelurahan', 'gempa', 'wartawan', 'luapan', 'banjirnya', 'kerusakan', 'pantauan', 'getaran', 'bogor', 'mengguyur', 'salahuddin', 'jakarta', 'laut', 'desa', 'gempa laut', 'jati jakarta', 'bermagnitudo', 'pembangunan', 'kramat', 'pertigaan', 'pungkasnya', 'langganan', 'lintang', 'diperparah', 'barat', 'hanyut', 'angin kelurahan', 'tulis', 'fadhila', 'digantung', 'banjirnya ngguyur', 'hulu', 'hek', 'benda', 'pusat gempa', 'mengimbau', 'kabupaten', 'jati', 'deras', 'skala', 'mengimbau warga', 'rahayu', 'kecamatan', 'hujan', 'kondisi', 'kondisi tanggul', 'puncak', 'kawasan', 'akibatnya', 'warga pembangunan', 'waspada', 'kecamatan malunda', 'andi', 'gambas', 'wib', 'warga', 'tumpah', 'tanggul kawasan', 'intensitas', 'benda benda', 'warga pemerintah', 'hek kramat', 'kawasan puncak', 'debit', 'pembangunan tanggul', 'debit air', 'pengerjaannya', 'penghubung', 'bergoyang', 'pemukiman', 'pusat', 'fadhiladetikcom', 'gempa 06', 'rumah', 'barat laut', 'mmi getaran', 'bambangan kecamatan', 'kemarin', 'tenggara']\n",
      "************************************************************************************************************************\n",
      "************************************************************************************************************************\n",
      "************************************************************************************************************************\n",
      "What\n",
      "\n"
     ]
    }
   ],
   "source": [
    "whatResultList = prepareWData(what_query, bow_list_what)\n",
    "print(\"What\")\n",
    "print()\n",
    "qeGabunganWhat = whatResultList[0]\n",
    "qeStatistikWhat = whatResultList[1]\n",
    "qeBoWWhat = whatResultList[2]\n",
    "qeGabunganDelimiterWhat = whatResultList[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation what\n",
    "def wCalculationWhat():\n",
    "    document_result = []\n",
    "    document_result_w = []\n",
    "\n",
    "    for i in range(0, len(document_text_test)-1):\n",
    "        hasilWhat = []\n",
    "\n",
    "        teks = df_total.iloc[i, -2]\n",
    "        tfidf_vectorizer = joblib.load('corpus/vectorizer/vectorizer.pkl')\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform([teks])\n",
    "        document_result_w.append(df_total.iloc[i, -2])\n",
    "\n",
    "        query_vec_what = tfidf_vectorizer.transform(qeGabunganWhat)\n",
    "        results_what = cosine_similarity(tfidf_matrix, query_vec_what).reshape((-1))\n",
    "\n",
    "        for key in qeGabunganDelimiterWhat:\n",
    "            cariW = re.findall(key, document_result_w[i])\n",
    "            if cariW:\n",
    "                hasilWhat.append(key)\n",
    "\n",
    "        document_result.append([i, qeGabunganWhat, qeStatistikWhat, qeBoWWhat, hasilWhat, results_what, 0, 0, teks])\n",
    "        # ------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    writer = pd.DataFrame(document_result, columns=['Data', 'QE Gabungan', 'QE Statistik', 'QE Bow', 'Hasil Query', 'Similarity', 'True Positive', 'True Negative','Skimming News'])\n",
    "    writer.to_csv('result/QE_Stat_V2_testing_result_what_new.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "wCalculationWhat()\n",
    "# wCalculationWho()\n",
    "# wCalculationWhen()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "905fc7ad738d4a245cabb3a2769a9f6f473849a63169dd5ebd94df3edb75ca90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
