{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-26 08:28:38,066 SequenceTagger predicts: Dictionary with 20 tags: <unk>, NOUN, PROPN, PUNCT, VERB, ADP, PRON, ADJ, NUM, DET, CCONJ, ADV, AUX, SCONJ, PART, SYM, X, INTJ, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "\n",
    "PATH = \"D:/Petakabar/whowherewhen/\"\n",
    "bencana = \"bencana/\"\n",
    "ekonomi = \"ekonomi/\"\n",
    "kecelakaan = \"kecelakaan/\"\n",
    "kesehatan = \"kesehatan/\"\n",
    "kriminalitas = \"kriminalitas/\"\n",
    "olahraga = \"olahraga/\"\n",
    "\n",
    "model_location = PATH\n",
    "\n",
    "news = pd.read_csv('perbandingan/berita-24-5.csv')\n",
    "tag_pos = SequenceTagger.load(model_location + 'best-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(berita, result=\"l\"):\n",
    "        s = str(berita)\n",
    "        # s = s.lower()\n",
    "        s = s.replace('\\n', ' ')\n",
    "        s = s.replace('\\r', ' ')\n",
    "        # s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "        if result == \"ll\":\n",
    "                tokens = [[token] for token in s.split(\" \") if token != \"\"]\n",
    "                # T = [t for t in tokens if (\n",
    "                #     (t in excluded_words) or (t not in NLTK_StopWords))]\n",
    "        elif result == \"asis\":\n",
    "                s = s.replace('  ', ' ')\n",
    "                return s\n",
    "        else:\n",
    "                tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "                # T = [t for t in tokens if (\n",
    "                #     (t in excluded_words) or (t not in NLTK_StopWords))]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE PICKLE of TAGGED\n",
    "\n",
    "from flair.data import Sentence\n",
    "import joblib\n",
    "\n",
    "len_docs = 110\n",
    "start = 0\n",
    "# len_docs = len(news)\n",
    "# print(len_docs)\n",
    "\n",
    "\n",
    "\n",
    "iter_through = ['bencana', 'ekonomi', 'kecelakaan', 'kesehatan', 'kriminalitas', 'olahraga']\n",
    "\n",
    "for topik_id, n in enumerate(iter_through):\n",
    "    tagged = []\n",
    "    berita = news[news['berita_topik_id'] == topik_id]\n",
    "    # find date\n",
    "    for i in range(start, start+len_docs):\n",
    "        text = berita['berita_desc'].iloc[i]\n",
    "        preprocessed_text = preprocessing(text, result='asis')\n",
    "        sentence = Sentence(preprocessed_text)\n",
    "        tag_pos.predict(sentence)\n",
    "        print(sentence.to_tagged_string())\n",
    "        tagged.append(sentence)\n",
    "\n",
    "    joblib.dump(tagged, PATH + 'tagged_' + n + '.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
