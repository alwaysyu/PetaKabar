{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "PATH = \"D:/Petakabar/perbandingan/\"\n",
    "bencana = \"bencana\"\n",
    "ekonomi = \"ekonomi\"\n",
    "kecelakaan = \"kecelakaan\"\n",
    "kesehatan = \"kesehatan\"\n",
    "kriminalitas = \"kriminalitas\"\n",
    "olahraga = \"olahraga\"\n",
    "\n",
    "news = pd.read_csv('perbandingan/berita-24-5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:/Petakabar/perbandingan/olahraga_test_unpreprocessed.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bencana\n",
    "berita = news[news['berita_topik_id'] == 1]\n",
    "df_train, df_test = train_test_split(berita, test_size=0.075, random_state=1)\n",
    "joblib.dump(df_train, PATH + bencana + '_train_unpreprocessed.pkl')\n",
    "joblib.dump(df_test, PATH + bencana + '_test_unpreprocessed.pkl')\n",
    "# Ekonomi\n",
    "berita = news[news['berita_topik_id'] == 2]\n",
    "df_train, df_test = train_test_split(berita, test_size=0.08, random_state=1)\n",
    "joblib.dump(df_train, PATH + ekonomi + '_train_unpreprocessed.pkl')\n",
    "joblib.dump(df_test, PATH + ekonomi + '_test_unpreprocessed.pkl')\n",
    "\n",
    "# Kecelakaan\n",
    "berita = news[news['berita_topik_id'] == 3]\n",
    "df_train, df_test = train_test_split(berita, test_size=0.06, random_state=1)\n",
    "joblib.dump(df_train, PATH + kecelakaan + '_train_unpreprocessed.pkl')\n",
    "joblib.dump(df_test, PATH + kecelakaan + '_test_unpreprocessed.pkl')\n",
    "\n",
    "# Kesehatan\n",
    "berita = news[news['berita_topik_id'] == 4]\n",
    "df_train, df_test = train_test_split(berita, test_size=0.11, random_state=1)\n",
    "joblib.dump(df_train, PATH + kesehatan + '_train_unpreprocessed.pkl')\n",
    "joblib.dump(df_test, PATH + kesehatan + '_test_unpreprocessed.pkl')\n",
    "\n",
    "# Kriminalitas\n",
    "berita = news[news['berita_topik_id'] == 5]\n",
    "df_train, df_test = train_test_split(berita, test_size=0.1, random_state=1)\n",
    "joblib.dump(df_train, PATH + kriminalitas + '_train_unpreprocessed.pkl')\n",
    "joblib.dump(df_test, PATH + kriminalitas + '_test_unpreprocessed.pkl')\n",
    "\n",
    "# Olahraga\n",
    "berita = news[news['berita_topik_id'] == 6]\n",
    "df_train, df_test = train_test_split(berita, test_size=0.1, random_state=1)\n",
    "joblib.dump(df_train, PATH + olahraga + '_train_unpreprocessed.pkl')\n",
    "joblib.dump(df_test, PATH + olahraga + '_test_unpreprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(berita, result=\"l\"):\n",
    "        s = str(berita)\n",
    "        # s = s.lower()\n",
    "        s = s.replace('\\n', ' ')\n",
    "        s = s.replace('\\r', ' ')\n",
    "        # s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "        if result == \"ll\":\n",
    "                tokens = [[token] for token in s.split(\" \") if token != \"\"]\n",
    "                # T = [t for t in tokens if (\n",
    "                #     (t in excluded_words) or (t not in NLTK_StopWords))]\n",
    "        elif result == \"asis\":\n",
    "                s = s.replace('  ', ' ')\n",
    "                return s\n",
    "        else:\n",
    "                tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "                # T = [t for t in tokens if (\n",
    "                #     (t in excluded_words) or (t not in NLTK_StopWords))]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bencana\n",
    "bencana_train = joblib.load(df_train, PATH + bencana + '_train_unpreprocessed.pkl')\n",
    "bencana_test = joblib.load(df_test, PATH + bencana + '_test_unpreprocessed.pkl')\n",
    "# Ekonomi\n",
    "ekonomi_train = joblib.load(df_train, PATH + ekonomi + '_train_unpreprocessed.pkl')\n",
    "ekonomi_test = joblib.load(df_test, PATH + ekonomi + '_test_unpreprocessed.pkl')\n",
    "\n",
    "# Kecelakaan\n",
    "kecelakaan_train = joblib.load(df_train, PATH + kecelakaan + '_train_unpreprocessed.pkl')\n",
    "kecelakaan_test = joblib.load(df_test, PATH + kecelakaan + '_test_unpreprocessed.pkl')\n",
    "\n",
    "# Kesehatan\n",
    "kesehatan_train = joblib.load(df_train, PATH + kesehatan + '_train_unpreprocessed.pkl')\n",
    "kesehatan_test = joblib.load(df_test, PATH + kesehatan + '_test_unpreprocessed.pkl')\n",
    "\n",
    "# Kriminalitas\n",
    "kriminalitas_train = joblib.load(df_train, PATH + kriminalitas + '_train_unpreprocessed.pkl')\n",
    "kriminalitas_test = joblib.load(df_test, PATH + kriminalitas + '_test_unpreprocessed.pkl')\n",
    "\n",
    "# Olahraga\n",
    "olahraga_train = joblib.load(df_train, PATH + olahraga + '_train_unpreprocessed.pkl')\n",
    "olahraga_test = joblib.load(df_test, PATH + olahraga + '_test_unpreprocessed.pkl')\n",
    "\n",
    "print(bencana, len(bencana_train), len(bencana_test), bencana_test.head(0))\n",
    "print(ekonomi, len(ekonomi_train), len(ekonomi_test), ekonomi_test.head(0))\n",
    "print(kecelakaan, len(kecelakaan_train), len(kecelakaan_test), kecelakaan_test.head(0))\n",
    "print(kesehatan, len(kesehatan_train), len(kesehatan_test), kesehatan_test.head(0))\n",
    "print(kriminalitas, len(kriminalitas_train), len(kriminalitas_test), kriminalitas_test.head(0))\n",
    "print(olahraga, len(olahraga_train), len(olahraga_test), olahraga_test.head(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# Model been trained using already tagged Corpus provided by Flair\n",
    "\n",
    "tag_pos = SequenceTagger.load(\"D:/Petakabar/whowherewhen/best-model.pt\")\n",
    "\n",
    "iter_through = ['bencana', 'ekonomi', 'kecelakaan', 'kesehatan', 'kriminalitas', 'olahraga']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1\n",
    "tagged = []\n",
    "for i in range(len(bencana_test)):\n",
    "    text = bencana_test['berita_desc'].iloc[i]\n",
    "    preprocessed_text = preprocessing(text)\n",
    "    sentence = Sentence(preprocessed_text)\n",
    "    tag_pos.predict(sentence)\n",
    "    tagged.append(sentence)\n",
    "\n",
    "joblib.dump('D:/PetaKabar/whowherewhen/test_tagged_'+ iter_through[x-1] +'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2\n",
    "tagged = []\n",
    "for i in range(len(ekonomi_test)):\n",
    "    text = ekonomi_test['berita_desc'].iloc[i]\n",
    "    preprocessed_text = preprocessing(text)\n",
    "    sentence = Sentence(preprocessed_text)\n",
    "    tag_pos.predict(sentence)\n",
    "    tagged.append(sentence)\n",
    "\n",
    "joblib.dump('D:/PetaKabar/whowherewhen/test_tagged_'+ iter_through[x-1] +'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 3\n",
    "tagged = []\n",
    "for i in range(len(kecelakaan_test)):\n",
    "    text = kecelakaan_test['berita_desc'].iloc[i]\n",
    "    preprocessed_text = preprocessing(text)\n",
    "    sentence = Sentence(preprocessed_text)\n",
    "    tag_pos.predict(sentence)\n",
    "    tagged.append(sentence)\n",
    "\n",
    "joblib.dump('D:/PetaKabar/whowherewhen/test_tagged_'+ iter_through[x-1] +'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 4\n",
    "tagged = []\n",
    "for i in range(len(kesehatan_test)):\n",
    "    text = kesehatan_test['berita_desc'].iloc[i]\n",
    "    preprocessed_text = preprocessing(text)\n",
    "    sentence = Sentence(preprocessed_text)\n",
    "    tag_pos.predict(sentence)\n",
    "    tagged.append(sentence)\n",
    "\n",
    "joblib.dump('D:/PetaKabar/whowherewhen/test_tagged_'+ iter_through[x-1] +'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 5\n",
    "tagged = []\n",
    "for i in range(len(kriminalitas_test)):\n",
    "    text = kriminalitas_test['berita_desc'].iloc[i]\n",
    "    preprocessed_text = preprocessing(text)\n",
    "    sentence = Sentence(preprocessed_text)\n",
    "    tag_pos.predict(sentence)\n",
    "    tagged.append(sentence)\n",
    "\n",
    "joblib.dump('D:/PetaKabar/whowherewhen/test_tagged_'+ iter_through[x-1] +'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 6\n",
    "tagged = []\n",
    "for i in range(len(olahraga_test)):\n",
    "    text = olahraga_test['berita_desc'].iloc[i]\n",
    "    preprocessed_text = preprocessing(text)\n",
    "    sentence = Sentence(preprocessed_text)\n",
    "    tag_pos.predict(sentence)\n",
    "    tagged.append(sentence)\n",
    "\n",
    "joblib.dump('D:/PetaKabar/whowherewhen/test_tagged_'+ iter_through[x-1] +'.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
