2023-06-22 16:40:27,595 ----------------------------------------------------------------------------------------------------
2023-06-22 16:40:27,597 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): WordEmbeddings(
      'id-crawl'
      (embedding): Embedding(1000000, 300)
    )
    (list_embedding_1): WordEmbeddings(
      'id'
      (embedding): Embedding(300686, 300)
    )
    (list_embedding_2): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.1, inplace=False)
        (encoder): Embedding(8823, 100)
        (rnn): LSTM(100, 2048)
      )
    )
    (list_embedding_3): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.1, inplace=False)
        (encoder): Embedding(8823, 100)
        (rnn): LSTM(100, 2048)
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=4696, out_features=4696, bias=True)
  (rnn): LSTM(4696, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=20, bias=True)
  (loss_function): ViterbiLoss()
  (crf): CRF()
)"
2023-06-22 16:40:27,599 ----------------------------------------------------------------------------------------------------
2023-06-22 16:40:27,601 Corpus: "Corpus: 4482 train + 559 dev + 557 test sentences"
2023-06-22 16:40:27,602 ----------------------------------------------------------------------------------------------------
2023-06-22 16:40:27,602 Parameters:
2023-06-22 16:40:27,602  - learning_rate: "0.100000"
2023-06-22 16:40:27,605  - mini_batch_size: "32"
2023-06-22 16:40:27,606  - patience: "3"
2023-06-22 16:40:27,606  - anneal_factor: "0.5"
2023-06-22 16:40:27,606  - max_epochs: "10"
2023-06-22 16:40:27,609  - shuffle: "True"
2023-06-22 16:40:27,609  - train_with_dev: "False"
2023-06-22 16:40:27,609  - batch_growth_annealing: "False"
2023-06-22 16:40:27,609 ----------------------------------------------------------------------------------------------------
2023-06-22 16:40:27,609 Model training base path: "D:\Petakabar\whowherewhen"
2023-06-22 16:40:27,614 ----------------------------------------------------------------------------------------------------
2023-06-22 16:40:27,614 Device: cpu
2023-06-22 16:40:27,614 ----------------------------------------------------------------------------------------------------
2023-06-22 16:40:27,630 Embeddings storage mode: cpu
2023-06-22 16:40:27,632 ----------------------------------------------------------------------------------------------------
2023-06-22 16:42:32,410 epoch 1 - iter 14/141 - loss 2.23435047 - time (sec): 124.78 - samples/sec: 77.39 - lr: 0.100000
2023-06-22 16:44:18,481 epoch 1 - iter 28/141 - loss 1.73144676 - time (sec): 230.85 - samples/sec: 83.47 - lr: 0.100000
2023-06-22 16:45:44,244 epoch 1 - iter 42/141 - loss 1.46248569 - time (sec): 316.61 - samples/sec: 90.82 - lr: 0.100000
2023-06-22 16:47:23,409 epoch 1 - iter 56/141 - loss 1.26387701 - time (sec): 415.77 - samples/sec: 92.77 - lr: 0.100000
2023-06-22 16:49:30,819 epoch 1 - iter 70/141 - loss 1.12124258 - time (sec): 543.18 - samples/sec: 89.46 - lr: 0.100000
2023-06-22 16:51:30,621 epoch 1 - iter 84/141 - loss 1.02091177 - time (sec): 662.99 - samples/sec: 88.08 - lr: 0.100000
2023-06-22 16:53:44,172 epoch 1 - iter 98/141 - loss 0.94025394 - time (sec): 796.54 - samples/sec: 86.13 - lr: 0.100000
2023-06-22 16:55:53,387 epoch 1 - iter 112/141 - loss 0.88177395 - time (sec): 925.75 - samples/sec: 84.49 - lr: 0.100000
2023-06-22 16:57:54,503 epoch 1 - iter 126/141 - loss 0.82974990 - time (sec): 1046.87 - samples/sec: 84.35 - lr: 0.100000
2023-06-22 16:59:42,820 epoch 1 - iter 140/141 - loss 0.78922492 - time (sec): 1155.19 - samples/sec: 84.48 - lr: 0.100000
2023-06-22 16:59:43,585 ----------------------------------------------------------------------------------------------------
2023-06-22 16:59:43,587 EPOCH 1 done: loss 0.7891 - lr 0.100000
2023-06-22 17:01:56,003 Evaluating as a multi-label problem: False
2023-06-22 17:01:56,227 DEV : loss 0.2612849771976471 - f1-score (micro avg)  0.9232
2023-06-22 17:01:56,281 BAD EPOCHS (no improvement): 0
2023-06-22 17:01:56,283 saving best model
2023-06-22 17:02:11,126 ----------------------------------------------------------------------------------------------------
2023-06-22 17:02:48,590 epoch 2 - iter 14/141 - loss 0.36476861 - time (sec): 37.46 - samples/sec: 263.41 - lr: 0.100000
2023-06-22 17:03:29,818 epoch 2 - iter 28/141 - loss 0.37961386 - time (sec): 78.69 - samples/sec: 250.71 - lr: 0.100000
2023-06-22 17:04:03,061 epoch 2 - iter 42/141 - loss 0.37367363 - time (sec): 111.93 - samples/sec: 261.44 - lr: 0.100000
2023-06-22 17:04:36,639 epoch 2 - iter 56/141 - loss 0.36960909 - time (sec): 145.51 - samples/sec: 265.41 - lr: 0.100000
2023-06-22 17:05:12,199 epoch 2 - iter 70/141 - loss 0.36856795 - time (sec): 181.07 - samples/sec: 266.30 - lr: 0.100000
2023-06-22 17:05:46,802 epoch 2 - iter 84/141 - loss 0.36786080 - time (sec): 215.68 - samples/sec: 267.51 - lr: 0.100000
2023-06-22 17:06:33,643 epoch 2 - iter 98/141 - loss 0.36715121 - time (sec): 262.52 - samples/sec: 259.05 - lr: 0.100000
2023-06-22 17:07:16,514 epoch 2 - iter 112/141 - loss 0.36504024 - time (sec): 305.39 - samples/sec: 254.80 - lr: 0.100000
2023-06-22 17:07:57,565 epoch 2 - iter 126/141 - loss 0.36215186 - time (sec): 346.44 - samples/sec: 253.02 - lr: 0.100000
2023-06-22 17:08:38,914 epoch 2 - iter 140/141 - loss 0.36058788 - time (sec): 387.79 - samples/sec: 251.63 - lr: 0.100000
2023-06-22 17:08:39,060 ----------------------------------------------------------------------------------------------------
2023-06-22 17:08:39,063 EPOCH 2 done: loss 0.3605 - lr 0.100000
2023-06-22 17:09:04,707 Evaluating as a multi-label problem: False
2023-06-22 17:09:04,817 DEV : loss 0.2169020026922226 - f1-score (micro avg)  0.9325
2023-06-22 17:09:04,881 BAD EPOCHS (no improvement): 0
2023-06-22 17:09:04,889 saving best model
2023-06-22 17:09:10,048 ----------------------------------------------------------------------------------------------------
2023-06-22 17:09:44,751 epoch 3 - iter 14/141 - loss 0.31761282 - time (sec): 34.70 - samples/sec: 277.18 - lr: 0.100000
2023-06-22 17:10:19,207 epoch 3 - iter 28/141 - loss 0.33142107 - time (sec): 69.16 - samples/sec: 281.21 - lr: 0.100000
2023-06-22 17:10:56,642 epoch 3 - iter 42/141 - loss 0.33133249 - time (sec): 106.59 - samples/sec: 276.00 - lr: 0.100000
2023-06-22 17:11:28,726 epoch 3 - iter 56/141 - loss 0.32734339 - time (sec): 138.68 - samples/sec: 282.33 - lr: 0.100000
2023-06-22 17:12:00,348 epoch 3 - iter 70/141 - loss 0.32374535 - time (sec): 170.30 - samples/sec: 285.65 - lr: 0.100000
2023-06-22 17:12:35,183 epoch 3 - iter 84/141 - loss 0.31992671 - time (sec): 205.13 - samples/sec: 283.76 - lr: 0.100000
2023-06-22 17:13:08,342 epoch 3 - iter 98/141 - loss 0.31775421 - time (sec): 238.29 - samples/sec: 285.19 - lr: 0.100000
2023-06-22 17:13:38,895 epoch 3 - iter 112/141 - loss 0.31586698 - time (sec): 268.85 - samples/sec: 289.09 - lr: 0.100000
2023-06-22 17:14:14,909 epoch 3 - iter 126/141 - loss 0.31475281 - time (sec): 304.86 - samples/sec: 285.97 - lr: 0.100000
2023-06-22 17:14:55,244 epoch 3 - iter 140/141 - loss 0.31475516 - time (sec): 345.20 - samples/sec: 282.65 - lr: 0.100000
2023-06-22 17:14:55,406 ----------------------------------------------------------------------------------------------------
2023-06-22 17:14:55,406 EPOCH 3 done: loss 0.3147 - lr 0.100000
2023-06-22 17:15:16,562 Evaluating as a multi-label problem: False
2023-06-22 17:15:16,637 DEV : loss 0.24424324929714203 - f1-score (micro avg)  0.9183
2023-06-22 17:15:16,699 BAD EPOCHS (no improvement): 1
2023-06-22 17:15:16,701 ----------------------------------------------------------------------------------------------------
2023-06-22 17:15:52,619 epoch 4 - iter 14/141 - loss 0.30952149 - time (sec): 35.92 - samples/sec: 270.08 - lr: 0.100000
2023-06-22 17:16:28,507 epoch 4 - iter 28/141 - loss 0.30728656 - time (sec): 71.81 - samples/sec: 262.25 - lr: 0.100000
2023-06-22 17:16:56,712 epoch 4 - iter 42/141 - loss 0.30060465 - time (sec): 100.01 - samples/sec: 286.47 - lr: 0.100000
2023-06-22 17:17:21,869 epoch 4 - iter 56/141 - loss 0.29778734 - time (sec): 125.17 - samples/sec: 304.48 - lr: 0.100000
2023-06-22 17:17:54,810 epoch 4 - iter 70/141 - loss 0.29315059 - time (sec): 158.11 - samples/sec: 303.08 - lr: 0.100000
2023-06-22 17:18:21,737 epoch 4 - iter 84/141 - loss 0.29240067 - time (sec): 185.04 - samples/sec: 309.78 - lr: 0.100000
2023-06-22 17:19:05,588 epoch 4 - iter 98/141 - loss 0.29284442 - time (sec): 228.89 - samples/sec: 295.50 - lr: 0.100000
2023-06-22 17:19:37,011 epoch 4 - iter 112/141 - loss 0.29074310 - time (sec): 260.31 - samples/sec: 298.19 - lr: 0.100000
2023-06-22 17:20:01,143 epoch 4 - iter 126/141 - loss 0.28919094 - time (sec): 284.44 - samples/sec: 306.59 - lr: 0.100000
2023-06-22 17:20:34,600 epoch 4 - iter 140/141 - loss 0.28884497 - time (sec): 317.90 - samples/sec: 306.86 - lr: 0.100000
2023-06-22 17:20:34,839 ----------------------------------------------------------------------------------------------------
2023-06-22 17:20:34,840 EPOCH 4 done: loss 0.2889 - lr 0.100000
2023-06-22 17:20:54,574 Evaluating as a multi-label problem: False
2023-06-22 17:20:54,660 DEV : loss 0.21068830788135529 - f1-score (micro avg)  0.931
2023-06-22 17:20:54,736 BAD EPOCHS (no improvement): 2
2023-06-22 17:20:54,744 ----------------------------------------------------------------------------------------------------
2023-06-22 17:21:22,876 epoch 5 - iter 14/141 - loss 0.25464828 - time (sec): 28.13 - samples/sec: 340.65 - lr: 0.100000
2023-06-22 17:21:48,963 epoch 5 - iter 28/141 - loss 0.26281946 - time (sec): 54.22 - samples/sec: 355.87 - lr: 0.100000
2023-06-22 17:22:14,380 epoch 5 - iter 42/141 - loss 0.26706226 - time (sec): 79.64 - samples/sec: 361.42 - lr: 0.100000
2023-06-22 17:22:36,543 epoch 5 - iter 56/141 - loss 0.27123579 - time (sec): 101.80 - samples/sec: 370.24 - lr: 0.100000
2023-06-22 17:22:59,164 epoch 5 - iter 70/141 - loss 0.27136209 - time (sec): 124.42 - samples/sec: 381.20 - lr: 0.100000
2023-06-22 17:23:28,505 epoch 5 - iter 84/141 - loss 0.27222092 - time (sec): 153.76 - samples/sec: 375.71 - lr: 0.100000
2023-06-22 17:24:00,248 epoch 5 - iter 98/141 - loss 0.27363577 - time (sec): 185.50 - samples/sec: 363.83 - lr: 0.100000
2023-06-22 17:24:27,199 epoch 5 - iter 112/141 - loss 0.27327489 - time (sec): 212.45 - samples/sec: 364.05 - lr: 0.100000
2023-06-22 17:24:55,013 epoch 5 - iter 126/141 - loss 0.27217319 - time (sec): 240.27 - samples/sec: 365.17 - lr: 0.100000
2023-06-22 17:25:23,416 epoch 5 - iter 140/141 - loss 0.27162660 - time (sec): 268.67 - samples/sec: 363.04 - lr: 0.100000
2023-06-22 17:25:23,655 ----------------------------------------------------------------------------------------------------
2023-06-22 17:25:23,656 EPOCH 5 done: loss 0.2716 - lr 0.100000
2023-06-22 17:25:40,223 Evaluating as a multi-label problem: False
2023-06-22 17:25:40,289 DEV : loss 0.2045883983373642 - f1-score (micro avg)  0.9333
2023-06-22 17:25:40,350 BAD EPOCHS (no improvement): 0
2023-06-22 17:25:40,351 saving best model
2023-06-22 17:25:44,583 ----------------------------------------------------------------------------------------------------
2023-06-22 17:26:14,837 epoch 6 - iter 14/141 - loss 0.25276310 - time (sec): 30.25 - samples/sec: 338.60 - lr: 0.100000
2023-06-22 17:26:49,922 epoch 6 - iter 28/141 - loss 0.24808203 - time (sec): 65.34 - samples/sec: 310.53 - lr: 0.100000
2023-06-22 17:27:20,345 epoch 6 - iter 42/141 - loss 0.25096601 - time (sec): 95.76 - samples/sec: 315.13 - lr: 0.100000
2023-06-22 17:27:50,673 epoch 6 - iter 56/141 - loss 0.25217358 - time (sec): 126.09 - samples/sec: 315.22 - lr: 0.100000
2023-06-22 17:28:20,659 epoch 6 - iter 70/141 - loss 0.25509738 - time (sec): 156.08 - samples/sec: 317.95 - lr: 0.100000
2023-06-22 17:28:50,821 epoch 6 - iter 84/141 - loss 0.25296349 - time (sec): 186.24 - samples/sec: 315.63 - lr: 0.100000
2023-06-22 17:29:20,780 epoch 6 - iter 98/141 - loss 0.25362687 - time (sec): 216.20 - samples/sec: 315.17 - lr: 0.100000
2023-06-22 17:29:50,576 epoch 6 - iter 112/141 - loss 0.25223587 - time (sec): 245.99 - samples/sec: 317.00 - lr: 0.100000
2023-06-22 17:30:21,540 epoch 6 - iter 126/141 - loss 0.25479361 - time (sec): 276.96 - samples/sec: 315.85 - lr: 0.100000
2023-06-22 17:30:50,535 epoch 6 - iter 140/141 - loss 0.25526351 - time (sec): 305.95 - samples/sec: 318.94 - lr: 0.100000
2023-06-22 17:30:50,674 ----------------------------------------------------------------------------------------------------
2023-06-22 17:30:50,675 EPOCH 6 done: loss 0.2552 - lr 0.100000
2023-06-22 17:31:07,721 Evaluating as a multi-label problem: False
2023-06-22 17:31:07,809 DEV : loss 0.20364481210708618 - f1-score (micro avg)  0.9322
2023-06-22 17:31:07,876 BAD EPOCHS (no improvement): 1
2023-06-22 17:31:07,877 ----------------------------------------------------------------------------------------------------
2023-06-22 17:31:37,113 epoch 7 - iter 14/141 - loss 0.23954868 - time (sec): 29.24 - samples/sec: 334.19 - lr: 0.100000
2023-06-22 17:32:09,762 epoch 7 - iter 28/141 - loss 0.24843766 - time (sec): 61.88 - samples/sec: 322.01 - lr: 0.100000
2023-06-22 17:32:39,324 epoch 7 - iter 42/141 - loss 0.24990585 - time (sec): 91.45 - samples/sec: 323.11 - lr: 0.100000
2023-06-22 17:33:08,768 epoch 7 - iter 56/141 - loss 0.24881604 - time (sec): 120.89 - samples/sec: 328.05 - lr: 0.100000
2023-06-22 17:33:37,346 epoch 7 - iter 70/141 - loss 0.24661978 - time (sec): 149.47 - samples/sec: 331.47 - lr: 0.100000
2023-06-22 17:34:06,918 epoch 7 - iter 84/141 - loss 0.24653751 - time (sec): 179.04 - samples/sec: 332.06 - lr: 0.100000
2023-06-22 17:34:31,767 epoch 7 - iter 98/141 - loss 0.24525081 - time (sec): 203.89 - samples/sec: 337.84 - lr: 0.100000
2023-06-22 17:35:00,722 epoch 7 - iter 112/141 - loss 0.24322182 - time (sec): 232.84 - samples/sec: 338.18 - lr: 0.100000
2023-06-22 17:35:27,663 epoch 7 - iter 126/141 - loss 0.24372868 - time (sec): 259.79 - samples/sec: 339.18 - lr: 0.100000
2023-06-22 17:36:01,919 epoch 7 - iter 140/141 - loss 0.24434335 - time (sec): 294.04 - samples/sec: 331.81 - lr: 0.100000
2023-06-22 17:36:02,091 ----------------------------------------------------------------------------------------------------
2023-06-22 17:36:02,098 EPOCH 7 done: loss 0.2445 - lr 0.100000
2023-06-22 17:36:24,582 Evaluating as a multi-label problem: False
2023-06-22 17:36:24,677 DEV : loss 0.18855087459087372 - f1-score (micro avg)  0.9393
2023-06-22 17:36:24,739 BAD EPOCHS (no improvement): 0
2023-06-22 17:36:24,741 saving best model
2023-06-22 17:36:28,655 ----------------------------------------------------------------------------------------------------
2023-06-22 17:37:01,845 epoch 8 - iter 14/141 - loss 0.24044385 - time (sec): 33.19 - samples/sec: 282.20 - lr: 0.100000
2023-06-22 17:37:38,806 epoch 8 - iter 28/141 - loss 0.22912910 - time (sec): 70.15 - samples/sec: 271.03 - lr: 0.100000
2023-06-22 17:38:17,171 epoch 8 - iter 42/141 - loss 0.23306753 - time (sec): 108.52 - samples/sec: 267.19 - lr: 0.100000
2023-06-22 17:38:59,928 epoch 8 - iter 56/141 - loss 0.23088426 - time (sec): 151.27 - samples/sec: 255.96 - lr: 0.100000
2023-06-22 17:39:37,242 epoch 8 - iter 70/141 - loss 0.23221670 - time (sec): 188.59 - samples/sec: 257.68 - lr: 0.100000
2023-06-22 17:40:12,005 epoch 8 - iter 84/141 - loss 0.23176676 - time (sec): 223.35 - samples/sec: 261.48 - lr: 0.100000
2023-06-22 17:40:46,951 epoch 8 - iter 98/141 - loss 0.23105786 - time (sec): 258.30 - samples/sec: 263.74 - lr: 0.100000
2023-06-22 17:41:23,228 epoch 8 - iter 112/141 - loss 0.23263189 - time (sec): 294.57 - samples/sec: 265.36 - lr: 0.100000
2023-06-22 17:41:58,595 epoch 8 - iter 126/141 - loss 0.23562520 - time (sec): 329.94 - samples/sec: 266.83 - lr: 0.100000
2023-06-22 17:42:35,423 epoch 8 - iter 140/141 - loss 0.23579341 - time (sec): 366.77 - samples/sec: 266.04 - lr: 0.100000
2023-06-22 17:42:35,591 ----------------------------------------------------------------------------------------------------
2023-06-22 17:42:35,592 EPOCH 8 done: loss 0.2358 - lr 0.100000
2023-06-22 17:42:57,555 Evaluating as a multi-label problem: False
2023-06-22 17:42:57,649 DEV : loss 0.18506895005702972 - f1-score (micro avg)  0.9415
2023-06-22 17:42:57,721 BAD EPOCHS (no improvement): 0
2023-06-22 17:42:57,723 saving best model
2023-06-22 17:43:01,615 ----------------------------------------------------------------------------------------------------
2023-06-22 17:43:35,303 epoch 9 - iter 14/141 - loss 0.21867346 - time (sec): 33.69 - samples/sec: 281.97 - lr: 0.100000
2023-06-22 17:44:14,670 epoch 9 - iter 28/141 - loss 0.22373685 - time (sec): 73.05 - samples/sec: 273.62 - lr: 0.100000
2023-06-22 17:44:45,456 epoch 9 - iter 42/141 - loss 0.21550163 - time (sec): 103.84 - samples/sec: 283.82 - lr: 0.100000
2023-06-22 17:45:16,682 epoch 9 - iter 56/141 - loss 0.21696191 - time (sec): 135.07 - samples/sec: 289.94 - lr: 0.100000
2023-06-22 17:45:57,024 epoch 9 - iter 70/141 - loss 0.21857592 - time (sec): 175.41 - samples/sec: 279.31 - lr: 0.100000
2023-06-22 17:46:46,420 epoch 9 - iter 84/141 - loss 0.21868655 - time (sec): 224.81 - samples/sec: 261.30 - lr: 0.100000
2023-06-22 17:47:26,748 epoch 9 - iter 98/141 - loss 0.22090466 - time (sec): 265.13 - samples/sec: 259.60 - lr: 0.100000
2023-06-22 17:48:04,931 epoch 9 - iter 112/141 - loss 0.22192030 - time (sec): 303.32 - samples/sec: 259.23 - lr: 0.100000
2023-06-22 17:48:45,386 epoch 9 - iter 126/141 - loss 0.22324792 - time (sec): 343.77 - samples/sec: 256.71 - lr: 0.100000
2023-06-22 17:49:22,366 epoch 9 - iter 140/141 - loss 0.22356234 - time (sec): 380.75 - samples/sec: 256.23 - lr: 0.100000
2023-06-22 17:49:22,553 ----------------------------------------------------------------------------------------------------
2023-06-22 17:49:22,555 EPOCH 9 done: loss 0.2236 - lr 0.100000
2023-06-22 17:49:47,802 Evaluating as a multi-label problem: False
2023-06-22 17:49:47,918 DEV : loss 0.18180228769779205 - f1-score (micro avg)  0.9416
2023-06-22 17:49:48,007 BAD EPOCHS (no improvement): 0
2023-06-22 17:49:48,008 saving best model
2023-06-22 17:49:52,135 ----------------------------------------------------------------------------------------------------
2023-06-22 17:50:34,377 epoch 10 - iter 14/141 - loss 0.22381844 - time (sec): 42.24 - samples/sec: 226.67 - lr: 0.100000
2023-06-22 17:51:11,845 epoch 10 - iter 28/141 - loss 0.21434465 - time (sec): 79.71 - samples/sec: 241.85 - lr: 0.100000
2023-06-22 17:51:46,946 epoch 10 - iter 42/141 - loss 0.21343156 - time (sec): 114.81 - samples/sec: 251.91 - lr: 0.100000
2023-06-22 17:52:28,970 epoch 10 - iter 56/141 - loss 0.21734170 - time (sec): 156.83 - samples/sec: 246.50 - lr: 0.100000
2023-06-22 17:53:05,539 epoch 10 - iter 70/141 - loss 0.21437004 - time (sec): 193.40 - samples/sec: 251.49 - lr: 0.100000
2023-06-22 17:53:41,842 epoch 10 - iter 84/141 - loss 0.21413823 - time (sec): 229.71 - samples/sec: 254.27 - lr: 0.100000
2023-06-22 17:54:16,597 epoch 10 - iter 98/141 - loss 0.21511400 - time (sec): 264.46 - samples/sec: 257.83 - lr: 0.100000
2023-06-22 17:55:01,856 epoch 10 - iter 112/141 - loss 0.21416108 - time (sec): 309.72 - samples/sec: 252.77 - lr: 0.100000
2023-06-22 17:55:35,184 epoch 10 - iter 126/141 - loss 0.21643001 - time (sec): 343.05 - samples/sec: 255.66 - lr: 0.100000
2023-06-22 17:56:12,281 epoch 10 - iter 140/141 - loss 0.21753586 - time (sec): 380.15 - samples/sec: 256.64 - lr: 0.100000
2023-06-22 17:56:12,465 ----------------------------------------------------------------------------------------------------
2023-06-22 17:56:12,466 EPOCH 10 done: loss 0.2175 - lr 0.100000
2023-06-22 17:56:35,528 Evaluating as a multi-label problem: False
2023-06-22 17:56:35,643 DEV : loss 0.20545212924480438 - f1-score (micro avg)  0.9348
2023-06-22 17:56:35,710 BAD EPOCHS (no improvement): 1
2023-06-22 17:56:40,045 ----------------------------------------------------------------------------------------------------
2023-06-22 17:56:46,398 SequenceTagger predicts: Dictionary with 20 tags: <unk>, NOUN, PROPN, PUNCT, VERB, ADP, PRON, ADJ, NUM, DET, CCONJ, ADV, AUX, SCONJ, PART, SYM, X, INTJ, <START>, <STOP>
2023-06-22 17:59:00,718 Evaluating as a multi-label problem: False
2023-06-22 17:59:00,810 0.9472	0.9472	0.9472	0.9472
2023-06-22 17:59:00,811 
Results:
- F-score (micro) 0.9472
- F-score (macro) 0.8873
- Accuracy 0.9472

By class:
              precision    recall  f1-score   support

        NOUN     0.9284    0.9140    0.9211      2511
       PROPN     0.9283    0.9579    0.9429      2162
       PUNCT     1.0000    1.0000    1.0000      1623
        VERB     0.9557    0.9595    0.9576      1258
         ADP     0.9655    0.9551    0.9603      1114
        PRON     0.9646    0.9736    0.9691       644
         ADJ     0.8667    0.8258    0.8458       488
         NUM     0.9895    0.9818    0.9856       384
       CCONJ     0.9917    0.9917    0.9917       362
         DET     0.9501    0.9501    0.9501       341
         ADV     0.8795    0.8439    0.8614       346
         AUX     0.9745    1.0000    0.9871       229
       SCONJ     0.8622    0.8711    0.8667       194
        PART     0.9175    1.0000    0.9570        89
         SYM     1.0000    1.0000    1.0000         6
           X     0.0000    0.0000    0.0000         5

    accuracy                         0.9472     11756
   macro avg     0.8859    0.8890    0.8873     11756
weighted avg     0.9466    0.9472    0.9468     11756

2023-06-22 17:59:00,812 ----------------------------------------------------------------------------------------------------
